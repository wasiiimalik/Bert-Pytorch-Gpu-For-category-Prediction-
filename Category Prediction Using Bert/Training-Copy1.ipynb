{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6578ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import category\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a44934e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class tqdm_notebook in module tqdm.notebook:\n",
      "\n",
      "class tqdm_notebook(tqdm.std.tqdm)\n",
      " |  tqdm_notebook(*args, **kwargs)\n",
      " |  \n",
      " |  Experimental IPython/Jupyter Notebook widget using tqdm!\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      tqdm_notebook\n",
      " |      tqdm.std.tqdm\n",
      " |      tqdm.utils.Comparable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Supports the usual `tqdm.tqdm` parameters as well as those listed below.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      display  : Whether to call `display(self.container)` immediately\n",
      " |          [default: True].\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Backward-compatibility to use: for x in tqdm(iterable)\n",
      " |  \n",
      " |  clear(self, *_, **__)\n",
      " |      Clear current bar display.\n",
      " |  \n",
      " |  close(self)\n",
      " |      Cleanup and (if leave=False) close the progressbar.\n",
      " |  \n",
      " |  display(self, msg=None, pos=None, close=False, bar_style=None, check_delay=True)\n",
      " |      Use `self.sp` to display `msg` in the specified `pos`.\n",
      " |      \n",
      " |      Consider overloading this function when inheriting to use e.g.:\n",
      " |      `self.some_frontend(**self.format_dict)` instead of `self.sp`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      msg  : str, optional. What to display (default: `repr(self)`).\n",
      " |      pos  : int, optional. Position to `moveto`\n",
      " |        (default: `abs(self.pos)`).\n",
      " |  \n",
      " |  reset(self, total=None)\n",
      " |      Resets to 0 iterations for repeated use.\n",
      " |      \n",
      " |      Consider combining with `leave=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      total  : int or float, optional. Total to use for the new bar.\n",
      " |  \n",
      " |  update(self, n=1)\n",
      " |      Manually update the progress bar, useful for streams\n",
      " |      such as reading files.\n",
      " |      E.g.:\n",
      " |      >>> t = tqdm(total=filesize) # Initialise\n",
      " |      >>> for current_buffer in stream:\n",
      " |      ...    ...\n",
      " |      ...    t.update(len(current_buffer))\n",
      " |      >>> t.close()\n",
      " |      The last line is highly recommended, but possibly not necessary if\n",
      " |      `t.update()` will be called in such a way that `filesize` will be\n",
      " |      exactly reached and printed.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n  : int or float, optional\n",
      " |          Increment to add to the internal counter of iterations\n",
      " |          [default: 1]. If using float, consider specifying `{n:.3f}`\n",
      " |          or similar in `bar_format`, or specifying `unit_scale`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      out  : bool or None\n",
      " |          True if a `display()` was triggered.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  status_printer(_, total=None, desc=None, ncols=None)\n",
      " |      Manage the printing of an IPython/Jupyter Notebook progress bar widget.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  colour\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  monitor = <TMonitor(Thread-8, started daemon 2632)>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tqdm.std.tqdm:\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_value, traceback)\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  moveto(self, n)\n",
      " |  \n",
      " |  refresh(self, nolock=False, lock_args=None)\n",
      " |      Force refresh the display of this bar.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      nolock  : bool, optional\n",
      " |          If `True`, does not lock.\n",
      " |          If [default: `False`]: calls `acquire()` on internal lock.\n",
      " |      lock_args  : tuple, optional\n",
      " |          Passed to internal lock's `acquire()`.\n",
      " |          If specified, will only `display()` if `acquire()` returns `True`.\n",
      " |  \n",
      " |  set_description(self, desc=None, refresh=True)\n",
      " |      Set/modify description of the progress bar.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      desc  : str, optional\n",
      " |      refresh  : bool, optional\n",
      " |          Forces refresh [default: True].\n",
      " |  \n",
      " |  set_description_str(self, desc=None, refresh=True)\n",
      " |      Set/modify description without ': ' appended.\n",
      " |  \n",
      " |  set_postfix(self, ordered_dict=None, refresh=True, **kwargs)\n",
      " |      Set/modify postfix (additional stats)\n",
      " |      with automatic formatting based on datatype.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ordered_dict  : dict or OrderedDict, optional\n",
      " |      refresh  : bool, optional\n",
      " |          Forces refresh [default: True].\n",
      " |      kwargs  : dict, optional\n",
      " |  \n",
      " |  set_postfix_str(self, s='', refresh=True)\n",
      " |      Postfix without dictionary expansion, similar to prefix handling.\n",
      " |  \n",
      " |  unpause(self)\n",
      " |      Restart tqdm timer from last print time.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tqdm.std.tqdm:\n",
      " |  \n",
      " |  external_write_mode(file=None, nolock=False) from builtins.type\n",
      " |      Disable tqdm within context and refresh tqdm when exits.\n",
      " |      Useful when writing to standard output stream\n",
      " |  \n",
      " |  get_lock() from builtins.type\n",
      " |      Get the global lock. Construct it if it does not exist.\n",
      " |  \n",
      " |  pandas(**tqdm_kwargs) from builtins.type\n",
      " |      Registers the current `tqdm` class with\n",
      " |          pandas.core.\n",
      " |          ( frame.DataFrame\n",
      " |          | series.Series\n",
      " |          | groupby.(generic.)DataFrameGroupBy\n",
      " |          | groupby.(generic.)SeriesGroupBy\n",
      " |          ).progress_apply\n",
      " |      \n",
      " |      A new instance will be create every time `progress_apply` is called,\n",
      " |      and each instance will automatically `close()` upon completion.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tqdm_kwargs  : arguments for the tqdm instance\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import pandas as pd\n",
      " |      >>> import numpy as np\n",
      " |      >>> from tqdm import tqdm\n",
      " |      >>> from tqdm.gui import tqdm as tqdm_gui\n",
      " |      >>>\n",
      " |      >>> df = pd.DataFrame(np.random.randint(0, 100, (100000, 6)))\n",
      " |      >>> tqdm.pandas(ncols=50)  # can use tqdm_gui, optional kwargs, etc\n",
      " |      >>> # Now you can use `progress_apply` instead of `apply`\n",
      " |      >>> df.groupby(0).progress_apply(lambda x: x**2)\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      <https://stackoverflow.com/questions/18603270/        progress-indicator-during-pandas-operations-python>\n",
      " |  \n",
      " |  set_lock(lock) from builtins.type\n",
      " |      Set the global lock.\n",
      " |  \n",
      " |  wrapattr(stream, method, total=None, bytes=True, **tqdm_kwargs) from builtins.type\n",
      " |      stream  : file-like object.\n",
      " |      method  : str, \"read\" or \"write\". The result of `read()` and\n",
      " |          the first argument of `write()` should have a `len()`.\n",
      " |      \n",
      " |      >>> with tqdm.wrapattr(file_obj, \"read\", total=file_obj.size) as fobj:\n",
      " |      ...     while True:\n",
      " |      ...         chunk = fobj.read(chunk_size)\n",
      " |      ...         if not chunk:\n",
      " |      ...             break\n",
      " |  \n",
      " |  write(s, file=None, end='\\n', nolock=False) from builtins.type\n",
      " |      Print a message via tqdm (without overlap with bars).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from tqdm.std.tqdm:\n",
      " |  \n",
      " |  __new__(cls, *_, **__)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  format_interval(t)\n",
      " |      Formats a number of seconds as a clock time, [H:]MM:SS\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      t  : int\n",
      " |          Number of seconds.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      out  : str\n",
      " |          [H:]MM:SS\n",
      " |  \n",
      " |  format_meter(n, total, elapsed, ncols=None, prefix='', ascii=False, unit='it', unit_scale=False, rate=None, bar_format=None, postfix=None, unit_divisor=1000, initial=0, colour=None, **extra_kwargs)\n",
      " |      Return a string-based progress bar given some parameters\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n  : int or float\n",
      " |          Number of finished iterations.\n",
      " |      total  : int or float\n",
      " |          The expected total number of iterations. If meaningless (None),\n",
      " |          only basic progress statistics are displayed (no ETA).\n",
      " |      elapsed  : float\n",
      " |          Number of seconds passed since start.\n",
      " |      ncols  : int, optional\n",
      " |          The width of the entire output message. If specified,\n",
      " |          dynamically resizes `{bar}` to stay within this bound\n",
      " |          [default: None]. If `0`, will not print any bar (only stats).\n",
      " |          The fallback is `{bar:10}`.\n",
      " |      prefix  : str, optional\n",
      " |          Prefix message (included in total width) [default: ''].\n",
      " |          Use as {desc} in bar_format string.\n",
      " |      ascii  : bool, optional or str, optional\n",
      " |          If not set, use unicode (smooth blocks) to fill the meter\n",
      " |          [default: False]. The fallback is to use ASCII characters\n",
      " |          \" 123456789#\".\n",
      " |      unit  : str, optional\n",
      " |          The iteration unit [default: 'it'].\n",
      " |      unit_scale  : bool or int or float, optional\n",
      " |          If 1 or True, the number of iterations will be printed with an\n",
      " |          appropriate SI metric prefix (k = 10^3, M = 10^6, etc.)\n",
      " |          [default: False]. If any other non-zero number, will scale\n",
      " |          `total` and `n`.\n",
      " |      rate  : float, optional\n",
      " |          Manual override for iteration rate.\n",
      " |          If [default: None], uses n/elapsed.\n",
      " |      bar_format  : str, optional\n",
      " |          Specify a custom bar string formatting. May impact performance.\n",
      " |          [default: '{l_bar}{bar}{r_bar}'], where\n",
      " |          l_bar='{desc}: {percentage:3.0f}%|' and\n",
      " |          r_bar='| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, '\n",
      " |            '{rate_fmt}{postfix}]'\n",
      " |          Possible vars: l_bar, bar, r_bar, n, n_fmt, total, total_fmt,\n",
      " |            percentage, elapsed, elapsed_s, ncols, nrows, desc, unit,\n",
      " |            rate, rate_fmt, rate_noinv, rate_noinv_fmt,\n",
      " |            rate_inv, rate_inv_fmt, postfix, unit_divisor,\n",
      " |            remaining, remaining_s, eta.\n",
      " |          Note that a trailing \": \" is automatically removed after {desc}\n",
      " |          if the latter is empty.\n",
      " |      postfix  : *, optional\n",
      " |          Similar to `prefix`, but placed at the end\n",
      " |          (e.g. for additional stats).\n",
      " |          Note: postfix is usually a string (not a dict) for this method,\n",
      " |          and will if possible be set to postfix = ', ' + postfix.\n",
      " |          However other types are supported (#382).\n",
      " |      unit_divisor  : float, optional\n",
      " |          [default: 1000], ignored unless `unit_scale` is True.\n",
      " |      initial  : int or float, optional\n",
      " |          The initial counter value [default: 0].\n",
      " |      colour  : str, optional\n",
      " |          Bar colour (e.g. 'green', '#00ff00').\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      out  : Formatted meter and stats, ready to display.\n",
      " |  \n",
      " |  format_num(n)\n",
      " |      Intelligent scientific notation (.3g).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n  : int or float or Numeric\n",
      " |          A Number.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      out  : str\n",
      " |          Formatted number.\n",
      " |  \n",
      " |  format_sizeof(num, suffix='', divisor=1000)\n",
      " |      Formats a number (greater than unity) with SI Order of Magnitude\n",
      " |      prefixes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num  : float\n",
      " |          Number ( >= 1) to format.\n",
      " |      suffix  : str, optional\n",
      " |          Post-postfix [default: ''].\n",
      " |      divisor  : float, optional\n",
      " |          Divisor between prefixes [default: 1000].\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      out  : str\n",
      " |          Number with Order of Magnitude SI unit postfix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tqdm.std.tqdm:\n",
      " |  \n",
      " |  format_dict\n",
      " |      Public API for read-only member access.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from tqdm.std.tqdm:\n",
      " |  \n",
      " |  monitor_interval = 10\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tqdm.utils.Comparable:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, other)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __gt__(self, other)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __le__(self, other)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tqdm.utils.Comparable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878a2bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting unique categores this will be the first function\n",
    "def pretraining(df,companyname,instanceid):\n",
    "    possible_labels = df.category.unique()\n",
    "    label_dict = {}\n",
    "    for index, possible_label in enumerate(possible_labels):\n",
    "        label_dict[possible_label] = index\n",
    "    category_dict = {}\n",
    "    count = 0\n",
    "    for category in label_dict.keys():\n",
    "        category_dict[count] = category\n",
    "        count = count + 1\n",
    "    category_file_name = \"category\" + companyname + str(instanceid) + \".txt\"\n",
    "    text_file = open(category_file_name, \"w\")\n",
    "    categories = json.dumps(category_dict)\n",
    "    text_file.write(categories)\n",
    "    text_file.close()\n",
    "    return category_dict\n",
    "\n",
    "\n",
    "def training(df,companyname,instanceid):\n",
    "    possible_labels = df.category.unique()\n",
    "    label_dict = {}\n",
    "    for index, possible_label in enumerate(possible_labels):\n",
    "        label_dict[possible_label] = index\n",
    "    df.category = df['category'].map(label_dict)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                    df.category.values, \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df.category.values)\n",
    "    df['data_type'] = ['not_set']*df.shape[0]\n",
    "    df.loc[X_train, 'data_type'] = 'train'\n",
    "    df.loc[X_val, 'data_type'] = 'val'\n",
    "    df.groupby(['category', 'data_type']).count()\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        do_lower_case=True\n",
    "    )\n",
    "    print(df)\n",
    "    print(\"df[df.data_type=='train'].question.values\",type(df[df.data_type=='train'].question.values))\n",
    "    \n",
    "    encoded_data_train = tokenizer.batch_encode_plus(\n",
    "        \n",
    "        df[df.data_type=='train'].question.values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    encoded_data_val = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type=='val'].question.values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids_train = encoded_data_train['input_ids']\n",
    "    attention_masks_train = encoded_data_train['attention_mask']\n",
    "    labels_train = torch.tensor(df[df.data_type=='train'].category.values)\n",
    "\n",
    "    input_ids_val = encoded_data_val['input_ids']\n",
    "    attention_masks_val = encoded_data_val['attention_mask']\n",
    "    labels_val = torch.tensor(df[df.data_type=='val'].category.values)\n",
    "\n",
    "    dataset_train = TensorDataset(input_ids_train, \n",
    "                                attention_masks_train,\n",
    "                                labels_train)\n",
    "\n",
    "    dataset_val = TensorDataset(input_ids_val, \n",
    "                                attention_masks_val,\n",
    "                            labels_val)\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "                                        'bert-base-uncased', \n",
    "                                        num_labels = len(label_dict),\n",
    "                                        output_attentions = False,\n",
    "                                        output_hidden_states = False\n",
    "                                        )\n",
    "    batch_size = 4\n",
    "\n",
    "    dataloader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        sampler=RandomSampler(dataset_train),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    dataloader_val = DataLoader(\n",
    "        dataset_val,\n",
    "        sampler=RandomSampler(dataset_val),\n",
    "        batch_size=32\n",
    "    )\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr = 1e-5,\n",
    "        eps = 1e-8\n",
    "    )\n",
    "    epochs = 1\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps = len(dataloader_train)*epochs\n",
    "    )\n",
    "    def f1_score_func(preds, labels):\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return f1_score(labels_flat, preds_flat, average = 'weighted')\n",
    "\n",
    "    def accuracy_per_class(preds, labels):\n",
    "        label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "        \n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        \n",
    "        for label in np.unique(labels_flat):\n",
    "            y_preds = preds_flat[labels_flat==label]\n",
    "            y_true = labels_flat[labels_flat==label]\n",
    "            print(f'Class: {label_dict_inverse[label]}')\n",
    "            print(f'Accuracy:{len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "            \n",
    "\n",
    "    seed_val = 17\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    def evaluate(dataloader_val):\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        loss_val_total = 0\n",
    "        predictions, true_vals = [], []\n",
    "        \n",
    "        for batch in tqdm(dataloader_val):\n",
    "            \n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            \n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                    'attention_mask': batch[1],\n",
    "                    'labels':         batch[2],\n",
    "                    }\n",
    "\n",
    "            with torch.no_grad():        \n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            loss_val_total += loss.item()\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = inputs['labels'].cpu().numpy()\n",
    "            predictions.append(logits)\n",
    "            true_vals.append(label_ids)\n",
    "        \n",
    "        loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "        \n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        true_vals = np.concatenate(true_vals, axis=0)\n",
    "                \n",
    "        return loss_val_avg, predictions, true_vals\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        model.train()\n",
    "        loss_train_total = 0\n",
    "        \n",
    "        progress_bar = tqdm(dataloader_train, \n",
    "                            desc='Epoch {:1d}'.format(epoch), \n",
    "                            leave=False, \n",
    "                            disable=False)\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            model.zero_grad()\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            inputs = {\n",
    "                'input_ids': batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'labels': batch[2]\n",
    "            }\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "            loss_train_total +=loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})     \n",
    "        \n",
    "        torch.save(model.state_dict(), f'BERT_ft_Epoch{epoch}.model')\n",
    "        \n",
    "        tqdm.write('\\nEpoch {epoch}')\n",
    "        \n",
    "        loss_train_avg = loss_train_total/len(dataloader_train)\n",
    "        tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "        \n",
    "        val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
    "        val_f1 = f1_score_func(predictions, true_vals)\n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        tqdm.write(f'F1 Score (weighted): {val_f1}')\n",
    "    Bert_model_Name = \"BertModel\" + companyname + str(instanceid) \n",
    "    torch.save(model,Bert_model_Name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa1dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"asktalos_dataset.csv\", encoding='cp1252')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c8eccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Unnamed: 0'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "406bfd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>difference between machine learning and artifi...</td>\n",
       "      <td>course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is the difference between machine learnin...</td>\n",
       "      <td>course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what are the job prospects for data science fo...</td>\n",
       "      <td>course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what do i get if i subscribe to this certificate?</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what are the things i should i expect from thi...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>greetings</td>\n",
       "      <td>greetings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>good morning</td>\n",
       "      <td>greetings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>gm</td>\n",
       "      <td>greetings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>hello</td>\n",
       "      <td>greetings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>morning!</td>\n",
       "      <td>greetings</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>511 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question   category\n",
       "0    difference between machine learning and artifi...     course\n",
       "1    what is the difference between machine learnin...     course\n",
       "2    what are the job prospects for data science fo...     course\n",
       "3    what do i get if i subscribe to this certificate?    general\n",
       "4    what are the things i should i expect from thi...    general\n",
       "..                                                 ...        ...\n",
       "506                                          greetings  greetings\n",
       "507                                       good morning  greetings\n",
       "508                                                 gm  greetings\n",
       "509                                              hello  greetings\n",
       "510                                           morning!  greetings\n",
       "\n",
       "[511 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3829c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              question  category data_type\n",
      "0    difference between machine learning and artifi...         0     train\n",
      "1    what is the difference between machine learnin...         0       val\n",
      "2    what are the job prospects for data science fo...         0     train\n",
      "3    what do i get if i subscribe to this certificate?         1     train\n",
      "4    what are the things i should i expect from thi...         1     train\n",
      "..                                                 ...       ...       ...\n",
      "506                                          greetings         6     train\n",
      "507                                       good morning         6     train\n",
      "508                                                 gm         6     train\n",
      "509                                              hello         6     train\n",
      "510                                           morning!         6       val\n",
      "\n",
      "[511 rows x 3 columns]\n",
      "df[df.data_type=='train'].question.values <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thewa\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\thewa\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29aa214f34e54f64b1272067d5d914d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a2e6422a1148ec9e8cb8ac22c696ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2900/3433386330.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minstanceid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpretraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcompanyname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minstanceid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcompanyname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minstanceid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2900/1183809720.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(df, companyname, instanceid)\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             \u001b[0mprogress_bar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'training_loss'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'{:.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'BERT_ft_Epoch{epoch}.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "companyname = \"Amazon\"\n",
    "instanceid = 2\n",
    "categories = pretraining(data,companyname,instanceid) \n",
    "training = training(data,companyname,instanceid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa9359d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b5fed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0dbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308148e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a623d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e609f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
